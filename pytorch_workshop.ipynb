{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced85796-78fc-49b9-b97f-424292761b1c",
   "metadata": {},
   "source": [
    "# pytorchで簡単にニューラルネットワークを作ろう"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08552c-f994-4814-9324-a09789046bd6",
   "metadata": {},
   "source": [
    "## ニューラルネットワークとは\n",
    "\n",
    "- できることを見ると\n",
    "  - 回帰問題（数値予測）と分類問題が解けるやつ\n",
    "  - 入力と期待される出力の組（教師データ）を与えればいい感じに補間してくれるやつ\n",
    "    - 損失関数は上手に設定する必要がある\n",
    "- 構成するパーツを見ると\n",
    "  - 人工ニューロンがいっぱい繋がったやつ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b19e53-0d30-4ef4-b249-6bcb57f23424",
   "metadata": {},
   "source": [
    "## 人工ニューロンとは（数式は覚えなくていい）\n",
    "\n",
    "- $d$ 個の入力（=入力ベクトル）$\\boldsymbol{x}=(1,x_1,\\dots,x_d)^{T}$ を受け、\n",
    "  重み付け和を計算し、適当な関数（活性化関数）に通して出力するやつ\n",
    "  - 単に内積で表せるようにダミーの $1$ を第0次元に付け足している（cf. 同次座標系）\n",
    "- ニューロンの持つパラメータは\n",
    "  - 重み $\\boldsymbol{w}=(w_0,w_1,\\dots,w_d)^{T}$\n",
    "  - 活性化関数 $f$\n",
    "    - しばしば同じ層のニューロンは同じ活性化関数を使う\n",
    "- 出力は $f(\\boldsymbol{w}^{T}\\boldsymbol{x})=f(w_0+w_1x_1+\\dots+w_dx_d)$\n",
    "\n",
    "![ニューラルネットワークは多数のニューロンが接続されている](./resources/figures/neural_network.png)\n",
    "![ニューロンの入出力](./resources/figures/artificial_neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d9919-f7c6-4569-8442-f7b7459b1333",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの万能近似定理（普遍性定理）\n",
    "\n",
    "3層ニューラルネットワークはいくつかの条件のもと、その重みを調節することによって出力の誤差を任意の正数まで減らすことができる\n",
    "\n",
    "現在は活性化関数はSigmoidalな関数でなくても良いとされている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c811f-4022-407b-ab62-6bb345330218",
   "metadata": {},
   "source": [
    "## ニューラルネットワークで分類問題を解くには\n",
    "\n",
    "$c$ クラス分類問題を解くならば、$c$ 出力のニューラルネットワークにする\n",
    "\n",
    "入力を与えてForward計算をしたとき、出力層において最大の値を取っているノードのクラスに分類されたとする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01b46d-8e5c-4ac3-9218-443e471b8d52",
   "metadata": {},
   "source": [
    "## 画像分類問題を解く\n",
    "\n",
    "### CIFAR-10とは\n",
    "\n",
    "32x32サイズのRGBカラー画像が、10クラス各6000個用意されたデータセット。\n",
    "\n",
    "### ニューラルネットワークに画像を入力する（一般論として）\n",
    "\n",
    "サイズ $A \\times B$、3チャンネル（RGB）固定であるとするならば、flattenして $3AB$ 次元のベクトルとして入力すれば良い\n",
    "\n",
    "### CIFAR-10の場合\n",
    "\n",
    "32x32 のRGB画像なので、入力は32x32x3=3072次元とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eef0aa0-c439-4ef9-bdca-48e147de71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299946  [    0/50000]\n",
      "loss: 1.948577  [ 5000/50000]\n",
      "loss: 2.126779  [10000/50000]\n",
      "loss: 1.857857  [15000/50000]\n",
      "loss: 1.899544  [20000/50000]\n",
      "loss: 1.654425  [25000/50000]\n",
      "loss: 1.673904  [30000/50000]\n",
      "loss: 2.124661  [35000/50000]\n",
      "loss: 1.670381  [40000/50000]\n",
      "loss: 1.508015  [45000/50000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 1.703821 \n",
      "\n",
      "[[179  33  82  27  43  20   4  34  97  43]\n",
      " [ 71 192  31  81  35  40  53  56 117 167]\n",
      " [ 91  20 158  94 134 109 106  74  23  12]\n",
      " [ 18  23  44 118  31 101  45  34  17  21]\n",
      " [ 63  43 147 119 179 112 135 142  41  41]\n",
      " [  4  18  45 103  23 140  44  33  24  15]\n",
      " [ 20  25  70  76  73  68 175  42   3  31]\n",
      " [ 28  23  42  44  40  49  13 169  23  31]\n",
      " [127  74  35  29  27  39  21  31 188  91]\n",
      " [ 26  62  11  31  10  15  13  57  42 156]]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.937411  [    0/50000]\n",
      "loss: 1.719023  [ 5000/50000]\n",
      "loss: 1.718871  [10000/50000]\n",
      "loss: 1.838299  [15000/50000]\n",
      "loss: 1.781732  [20000/50000]\n",
      "loss: 1.571219  [25000/50000]\n",
      "loss: 1.882040  [30000/50000]\n",
      "loss: 1.643167  [35000/50000]\n",
      "loss: 1.472239  [40000/50000]\n",
      "loss: 1.550858  [45000/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.5%, Avg loss: 1.609187 \n",
      "\n",
      "[[179  20  64  20  35   9   2  14  64  18]\n",
      " [ 82 194  46  40  31  40  25  49  96 146]\n",
      " [ 60   7 119  47  75  49  24  40   9   3]\n",
      " [ 31  31  81 166  48 131  66  68  27  45]\n",
      " [ 37  19 113  61 166  65  66  81  22  12]\n",
      " [  5  15  37  79  18 129  16  30  13  12]\n",
      " [ 48  35 152 135 150 135 193  70  25  38]\n",
      " [ 42  26  63  59  78  75  31 184  13  46]\n",
      " [126  60  34  28  34  30  14  21 192  60]\n",
      " [ 54 103  21  46  21  25  17  65  59 183]]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.582107  [    0/50000]\n",
      "loss: 1.592885  [ 5000/50000]\n",
      "loss: 1.888528  [10000/50000]\n",
      "loss: 1.424587  [15000/50000]\n",
      "loss: 1.461702  [20000/50000]\n",
      "loss: 1.595936  [25000/50000]\n",
      "loss: 1.360711  [30000/50000]\n",
      "loss: 1.708357  [35000/50000]\n",
      "loss: 1.582886  [40000/50000]\n",
      "loss: 1.531281  [45000/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.1%, Avg loss: 1.565276 \n",
      "\n",
      "[[187  49  78  26  46  17   8  35  92  48]\n",
      " [ 27 190  23  15  12  15  15  14  54 104]\n",
      " [ 45  12 116  51  61  56  31  35  14   4]\n",
      " [ 50  44 109 182  88 160 121  86  47  50]\n",
      " [ 57  22 150  69 181  73 124  94  31  17]\n",
      " [ 11  24  46 100  38 151  50  51  20  17]\n",
      " [ 15  13  70  54  83  53 183  25   7  16]\n",
      " [ 63  42  69  47  86  71  39 190  22  53]\n",
      " [109  60  26  22  22  29  13  12 186  41]\n",
      " [ 55 118  16  55  22  23  19  43  90 187]]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.932118  [    0/50000]\n",
      "loss: 1.516098  [ 5000/50000]\n",
      "loss: 1.809408  [10000/50000]\n",
      "loss: 1.504609  [15000/50000]\n",
      "loss: 1.840265  [20000/50000]\n",
      "loss: 1.303722  [25000/50000]\n",
      "loss: 1.398845  [30000/50000]\n",
      "loss: 1.620591  [35000/50000]\n",
      "loss: 1.321155  [40000/50000]\n",
      "loss: 1.583890  [45000/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.9%, Avg loss: 1.569686 \n",
      "\n",
      "[[182  19  66  21  56  18   5  41  59  32]\n",
      " [ 93 196  55  65  38  44  59  62 101 170]\n",
      " [ 53   9 134  62  61  78  31  50  16   8]\n",
      " [ 27  19  87 172  61 153  89  70  32  35]\n",
      " [ 44  17 142  85 174  89 135 106  27  19]\n",
      " [  3   4  25  51  15 115  13  27   1   4]\n",
      " [ 26  12 102  88  95  82 181  54   8  32]\n",
      " [ 23  13  43  35  50  48  10 166   7  30]\n",
      " [130  67  48  45  35  59  28  32 192  70]\n",
      " [ 31  53  11  36  11  17   6  45  31 158]]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.532783  [    0/50000]\n",
      "loss: 1.386791  [ 5000/50000]\n",
      "loss: 1.423054  [10000/50000]\n",
      "loss: 1.422061  [15000/50000]\n",
      "loss: 1.326053  [20000/50000]\n",
      "loss: 1.887877  [25000/50000]\n",
      "loss: 1.560863  [30000/50000]\n",
      "loss: 1.098363  [35000/50000]\n",
      "loss: 1.509758  [40000/50000]\n",
      "loss: 1.612357  [45000/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.554798 \n",
      "\n",
      "[[185  46  90  48  59  27  22  46  76  48]\n",
      " [ 33 190  31  20  10  15  16  19  50 113]\n",
      " [ 25   9 144  43 110  53  65  30   9   3]\n",
      " [ 22  21  80 156  61 125 106  54  22  35]\n",
      " [  7   2  26   7  97   7  31  15   4   2]\n",
      " [ 34  35 113 150  96 182 100  82  25  21]\n",
      " [ 14  16  74  50 101  56 176  17   6  24]\n",
      " [ 54  34  86  71 136  94  61 192  24  63]\n",
      " [136  99  46  48  44  61  33  32 195  68]\n",
      " [ 38 116  17  51  12  21  15  40  55 186]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "# 全結合 3-Layer FFN の定義\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, input: int, hidden: int, output: int) -> None:\n",
    "        # モデルの重みの定義\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.__fc1 = nn.Linear(input, hidden)\n",
    "        self.__fc2 = nn.Linear(hidden, output)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 入力から出力を計算する\n",
    "        x = self.flatten(x)\n",
    "        x = self.__fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.__fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_loop(model, dataloader, loss_fn, optimizer) -> None:\n",
    "    # 学習用関数の定義\n",
    "    num_classes = 10\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        \n",
    "def test_loop(model, dataloader, loss_fn) -> None:\n",
    "    # 評価用関数の定義\n",
    "    num_classes = 10\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            confusion_matrix[pred.argmax(1).detach().numpy(), y] += 1\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(confusion_matrix)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # CIFAR-10 データセットを読み込む\n",
    "    cifar10_data_train = torchvision.datasets.CIFAR10('./', train=True,  download=True, transform=torchvision.transforms.ToTensor())\n",
    "    cifar10_data_test  = torchvision.datasets.CIFAR10('./', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "    # DataLoader の設定\n",
    "    dataloader_train = torch.utils.data.DataLoader(cifar10_data_train, batch_size=50, shuffle=True)\n",
    "    dataloader_test  = torch.utils.data.DataLoader(cifar10_data_test,  batch_size=50, shuffle=True)\n",
    "    \n",
    "    ffn = FFN(input=3072, hidden=1000, output=10)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(ffn.parameters())\n",
    "    \n",
    "    epochs = 5\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "        train_loop(ffn, dataloader_train, loss_fn, optimizer)\n",
    "        test_loop(ffn, dataloader_test, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c60774-fe2d-4891-8dbc-7c43828a97dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
